# https://www.robotstxt.org/robotstxt.html

# Règles générales pour tous les robots
User-agent: *
Allow: /
# Limitation du taux d'exploration - attendre 3 secondes entre chaque requête
Crawl-delay: 3

# Pages privées - interdites à tous les robots
Disallow: /me/
Disallow: /cart/
Disallow: /address/
Disallow: /shipping/
Disallow: /api/

# Règles spécifiques pour Google (qui ignore Crawl-delay mais utilise sa propre gestion)
User-agent: Googlebot
Allow: /
Disallow: /me/
Disallow: /cart/
Disallow: /address/
Disallow: /shipping/
Disallow: /api/
# Note: Google utilise Search Console pour la gestion du taux de crawl, pas Crawl-delay

# Règles spécifiques pour Bing
User-agent: Bingbot
Allow: /
Disallow: /me/
Disallow: /cart/
Disallow: /address/
Disallow: /shipping/
Disallow: /api/
Crawl-delay: 2

# Règles pour les robots spécialisés qui pourraient être plus intensifs
User-agent: AhrefsBot
User-agent: SemrushBot
User-agent: MJ12bot
Crawl-delay: 10
Disallow: /me/
Disallow: /cart/
Disallow: /address/
Disallow: /shipping/
Disallow: /api/

# Allow product and category pages (explicitement, pour plus de clarté)
Allow: /product/
Allow: /?keyword=
Allow: /?category=

# Sitemaps avec URLs relatives
# Avantage: fonctionne indépendamment du domaine ou environnement (dev, staging, prod)
Sitemap: /sitemap.xml
Sitemap: /server-sitemap.xml